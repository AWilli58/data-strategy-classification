{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing Documents with Similarity Scores\n",
    "The task is to identify the most relevant working group for each instance of feedback collected by OMB's Federal Data Strategy Group. Each instance of feedback and each of the four working groups is represented by a string description. Relevancy between instance-working-group combinations will be calculated using a similarity score after each textual element has been vectorized using word embeddings.\n",
    "\n",
    "The output is a spreadsheet containing all of the feedback instances along with similarity scores for each of the four working groups:\n",
    " - Enterprise Data Governance \n",
    " - Access, Use and Augmentation \n",
    " - Decision-Making and Accountability\n",
    " - Commercialization, Innovation, and Public Use \n",
    " \n",
    "The higher the similarity score, the more relevant the working group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.23) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import enchant\n",
    "import contractions\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity, polynomial_kernel\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'Federal Data Strategy Comments.xlsx',usecols=[1])\n",
    "keywords = pd.read_excel('Working_Group_Terms.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspect the Data\n",
    "Here, we'll visualy inspect the data to get a feel for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I work as part of the team behind DataUSA.io, an open data visualization website which relies heavily on data from federal government sources (e.g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#3 has 5 verbs\\n\\n#4 has 4 verbs, 2 adverbs, and 7 nouns and is basically a reiteration of OMB Circular A-130 and all the laws it implements.\\n\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Integrate LEHD Data with Local/Regional Transportation data to develop more robust and timely \"Access to Jobs\" metrics. \\n\\nFor example: a low-inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Previous engagement on government technology and data policy has occurred on GitHub -- with both the code and content available to all, and the fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is noticeable that there are no mentions of open data among the strategic principles, as this has been a strong aspect of previous U.S. governm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                Instance\n",
       "0  I work as part of the team behind DataUSA.io, an open data visualization website which relies heavily on data from federal government sources (e.g...\n",
       "1  #3 has 5 verbs\\n\\n#4 has 4 verbs, 2 adverbs, and 7 nouns and is basically a reiteration of OMB Circular A-130 and all the laws it implements.\\n\\n#...\n",
       "2  Integrate LEHD Data with Local/Regional Transportation data to develop more robust and timely \"Access to Jobs\" metrics. \\n\\nFor example: a low-inc...\n",
       "3  Previous engagement on government technology and data policy has occurred on GitHub -- with both the code and content available to all, and the fe...\n",
       "4  It is noticeable that there are no mentions of open data among the strategic principles, as this has been a strong aspect of previous U.S. governm..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth',150)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instance    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Working Groups\n",
    "There are $4$ working groups:\n",
    " - Enterprise Data Governance\n",
    " - Access, Use and Augmentation\n",
    " - Decision-Making and Accountability\n",
    " - Commercialization, Innovation, and Public Use\n",
    "\n",
    "Each of the $4$ working groups has a number of topics within it. Each of these topics then has a description. Our task is to assign comments to each of the working groups based on how similar the comment is to each working group's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Description</th>\n",
       "      <th>WG1</th>\n",
       "      <th>WG2</th>\n",
       "      <th>WG3</th>\n",
       "      <th>WG4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Governance:</td>\n",
       "      <td>Data Governance ensures that data assets are formally managed. A data governance model for the federal government establishes clarity, agreement a...</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Governance: Communications</td>\n",
       "      <td>is the process of using computing and communication technologies to transfer data from one place to another, and vice versa. It enables the moveme...</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Governance: Data Handling Ethics</td>\n",
       "      <td>in an ethical manner is necessary to the long-term success of any organization that wants to get value from its data.</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Governance: Data Principles &amp; Policies</td>\n",
       "      <td>fundamental rules governing the creation, acquisition, management, integrity, security, quality, and use of data and information</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Governance: Issues Management</td>\n",
       "      <td>is a process of removing or reducing the impact of obstacles that prevent effective use of data. Issue management includes identification, definit...</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Topic  \\\n",
       "0                            Data Governance:    \n",
       "1              Data Governance: Communications   \n",
       "2        Data Governance: Data Handling Ethics   \n",
       "3  Data Governance: Data Principles & Policies   \n",
       "4           Data Governance: Issues Management   \n",
       "\n",
       "                                                                                                                                             Description  \\\n",
       "0  Data Governance ensures that data assets are formally managed. A data governance model for the federal government establishes clarity, agreement a...   \n",
       "1  is the process of using computing and communication technologies to transfer data from one place to another, and vice versa. It enables the moveme...   \n",
       "2                                  in an ethical manner is necessary to the long-term success of any organization that wants to get value from its data.   \n",
       "3                       fundamental rules governing the creation, acquisition, management, integrity, security, quality, and use of data and information   \n",
       "4  is a process of removing or reducing the impact of obstacles that prevent effective use of data. Issue management includes identification, definit...   \n",
       "\n",
       "  WG1  WG2  WG3  WG4  \n",
       "0   x  NaN  NaN  NaN  \n",
       "1   x  NaN  NaN  NaN  \n",
       "2   x  NaN  NaN  NaN  \n",
       "3   x  NaN  NaN  NaN  \n",
       "4   x  NaN  NaN  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic           1\n",
       "Description    16\n",
       "WG1            64\n",
       "WG2            51\n",
       "WG3            55\n",
       "WG4            80\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning and Normalizing the Text\n",
    "In this step, we want to clean the text so that it only includes meaningful information. That means we need to:\n",
    " - strip html tags\n",
    " - strip urls\n",
    " - strip email addresses\n",
    " - strip nonsensical words that include numbers in them or are more than 17 characters long\n",
    " - expand contractions (e.g. can't becomes cannot)\n",
    " - strip misspellings (optional)\n",
    " - tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc, spellcheck=True):\n",
    "    \n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    \n",
    "    def strip_html_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text()\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_urls(text):\n",
    "        #url regex\n",
    "        url_re = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\")\n",
    "        stripped_text = url_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_emails(text):\n",
    "        #email address regex\n",
    "        email_re = re.compile(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)')\n",
    "        stripped_text = email_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_nonsense(text):\n",
    "        # leave words that are at least three characters long, do not contain a number, and are no more \n",
    "        # than 17 chars long\n",
    "        no_nonsense = re.findall(r'\\b[a-z][a-z][a-z]+\\b',text)\n",
    "        stripped_text = ' '.join(w for w in no_nonsense if w != 'nan' and len(w) <= 17)\n",
    "        return stripped_text\n",
    "\n",
    "    def expand_contractions(text, contraction_mapping=contractions.contractions_dict):\n",
    "\n",
    "            contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                              flags=re.IGNORECASE|re.DOTALL)\n",
    "            def expand_match(contraction):\n",
    "                match = contraction.group(0)\n",
    "                first_char = match[0]\n",
    "                if contraction_mapping.get(match):\n",
    "                    expanded_contraction = contraction_mapping.get(match)\n",
    "                else:\n",
    "                    expanded_contraction = contraction_mapping.get(match.lower())\n",
    "                if expanded_contraction:\n",
    "                    expanded_contraction = first_char+expanded_contraction[1:]\n",
    "                    return expanded_contraction\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "            expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "            return expanded_text\n",
    "\n",
    "    def strip_misspellings(text):\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        words_to_add = ['api','git','github','apis']\n",
    "        for w in words_to_add:\n",
    "            d.add(w)\n",
    "        \n",
    "        \n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        non_dict_words = set([word for word in tokens if d.check(word) is False and re.match('^[a-zA-Z ]*$',word)])\n",
    "        stripped_text = \" \".join([x for x in tokens if x not in non_dict_words])\n",
    "        return stripped_text\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = \" \".join([word for word in tokenizer.tokenize(doc) if word not in en_stop])\n",
    "    contraction_free = expand_contractions(doc)\n",
    "    tag_free = strip_html_tags(contraction_free)\n",
    "    url_free = strip_urls(tag_free)\n",
    "    email_free = strip_emails(url_free)\n",
    "    if spellcheck:\n",
    "        misspelling_free = strip_misspellings(email_free)\n",
    "        normalized = strip_nonsense(misspelling_free)\n",
    "\n",
    "    else:\n",
    "        normalized = strip_nonsense(email_free)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Clean the Feedback Instances\n",
    "Below, we'll `apply` the cleaning function to the feedback column in the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a crude, time-consuming approach TODO: simplify/optimize\n",
    "instance_corpus = data['Instance'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the first comment before the cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I work as part of the team behind DataUSA.io, an open data visualization website which relies heavily on data from federal government sources (e.g. Census, BLS, BEA, CDC). I'm glad to see principle #4 reference interoperability, because that has been one of our biggest challenges as users of federal data. In particular around consistent use of standard classification systems (for industries, occupations, courses of study, products etc). Many times agencies will talk about the same types of things but will use slightly different coding schemes that deviate from the standard classification systems. Working to find ways to preserve privacy while also maintaining adherence to standard classification systems is vital to users that are trying to inter-link datasets and unlock the full potential of the data.\n"
     ]
    }
   ],
   "source": [
    "print(data['Instance'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's that same comment after the cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work part team behind datausa open data visualization website relies heavily data federal government sources census glad see principle reference interoperability one biggest challenges users federal data particular around consistent use standard classification systems industries occupations courses study products etc many times agencies talk types things use slightly different coding schemes deviate standard classification systems working find ways preserve privacy also maintaining adherence standard classification systems vital users trying inter link datasets unlock full potential data\n"
     ]
    }
   ],
   "source": [
    "print(instance_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean the Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first concatenate the topic to its description\n",
    "keywords['Text'] = keywords['Topic'].fillna(value = \"\").astype(str) +\\\n",
    "                   \" \" + keywords['Description'].fillna(value = \"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this concatenated text, replacing np.nan with empty strings\n",
    "keywords['Clean Text'] = keywords['Text'].apply(lambda x: clean(x))\n",
    "keywords['Clean Text'].replace(r'',np.nan,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_corpus = keywords['Clean Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a working group topic description before the cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Governance: Operating Framework clarifies, and establishes, and executes agreements over the specification of decision rights, accountability frameworks, and data management standards, in order to encourage standardized data management during the data lifecycle.\n"
     ]
    }
   ],
   "source": [
    "print(keywords['Text'].iloc[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's that same text after the cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data governance operating framework clarifies establishes executes agreements specification decision rights accountability frameworks data management standards order encourage standardized data management data'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_corpus[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Combine the Corpora\n",
    "In this step, we'll combine the keyword and feedback corpora into a single corpus. We'll use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Combined Corpus\n",
    "corpus = pd.concat([keyword_corpus,instance_corpus]).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Vectorization\n",
    "You can't do math with letters, so you need a way to convert words into numbers. This process is called vectorizatio.\n",
    "\n",
    "In Natural Language Processing (NLP), there's a variety of methods that map words into vectors that contains numeric values so that machines can do math with those vectors. We'll look at two methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The Traditional Text Vectorization Approach\n",
    "The traditional method of representing words as vectors is to **one-hot encode** them. In this process, the length of the vector is equal to the size of the total unique vocabulary in the corpora. Conventionally, these unique words are encoded in alphabetical order. Namely, you should expect the one-hot vectors for words starting with “a” with target “1” of lower index, while those for words beginning with “z” with target “1” of higher index.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ULfyiWPKgWceCqyZeDTl0g.png\" height = 450 width=450>\n",
    "\n",
    "Although this vectorization is simple and easy to implement, there are several issues:\n",
    " - you cannot infer relationships between two words. For instance, although the words “endure” and “tolerate”, have similar meanings, their targets “1” are far from each other within each word's vector. \n",
    " - the vectors are very sparse, as there are numerous redundant 0's in the vectors. This means that we are wasting a lot of space in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Word Embeddings\n",
    "A **word embedding** is a type of mapping that allows words with similar meanings to have similar vector representations. A key assumption in this mapping is the **distributional hypothesis**, which states that words with similar meanings will occur with similar neighbors.\n",
    "\n",
    "Creating a word embedding involves training a neural network to learn the surrounding words that represent every target word within a corpus.\n",
    "\n",
    "The vectors returned by this neural network can then mathematically describe the relationship between words. The vectors obtained by subtracting two related words sometimes express a meaningful concept such as gender or verb tense, as shown in the following figure (dimensionality reduced).\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/linear-relationships.png\" height=500 width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create Word Embeddings for the Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Learning word embeddings using the FastText Model...\n",
      "\tDone learning word embeddings using the FastText Model.\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokens = [word_tokenize(s) for s in corpus]\n",
    "#### Create Fast Text Model ####\n",
    "# Set values for various parameters\n",
    "print(\"=\"*80)\n",
    "print(\"Learning word embeddings using the FastText Model...\")\n",
    "feature_size = 300    # Word vector dimensionality\n",
    "window_context = 50   # Context window size\n",
    "min_word_count = 3    # Minimum word count\n",
    "sample = 1e-3         # Downsample setting for frequent words\n",
    "\n",
    "ft_model = gensim.models.fasttext.FastText(tokens, size=feature_size, window=window_context,\n",
    "                                           min_count=min_word_count,sample=sample, sg=1, iter=50)\n",
    "ft_embedding = {w: vec for w, vec in zip(ft_model.wv.vocab.keys(), ft_model.wv.vectors)}\n",
    "print(\"\\tDone learning word embeddings using the FastText Model.\")\n",
    "print(\"_\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained a word embedding model on our corpus, let's see the vector representation for the word `privacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15132828  0.02029222  0.01786475  0.25283185 -0.03917486  0.12799788\n",
      "  0.08950055  0.12108936  0.09062934 -0.25107852  0.01070699  0.04959331\n",
      "  0.21600719 -0.06086767 -0.08672404 -0.10897539 -0.12762794  0.11744515\n",
      "  0.03871953 -0.2254867  -0.48760262 -0.11121111 -0.16992103 -0.17141436\n",
      " -0.5840545  -0.30030853  0.36021912 -0.12021217  0.39940685 -0.27369046\n",
      " -0.17695956  0.16677961  0.23014732  0.30586562  0.19671768 -0.27680472\n",
      " -0.4058172  -0.05920848 -0.07532392  0.42110482 -0.17636825  0.04961498\n",
      " -0.18312755  0.15576707 -0.08817371 -0.11760711 -0.03076152  0.38899297\n",
      " -0.3006116  -0.05366548 -0.04566906  0.07428943 -0.32678893  0.13739768\n",
      " -0.12024488 -0.05655747  0.27413812  0.20844308 -0.00129339 -0.08231981\n",
      "  0.03263489 -0.07868057  0.23347257  0.24552996 -0.25580892 -0.44712627\n",
      " -0.08145075  0.09248609 -0.22897635  0.11709151  0.09525267 -0.6060213\n",
      "  0.5456053   0.23342383 -0.3132998  -0.35040933 -0.08005612  0.22636233\n",
      "  0.30704     0.16314673 -0.216378   -0.1393235  -0.153861   -0.15807503\n",
      "  0.09149586  0.15088452  0.06906798  0.09709128  0.24664032 -0.09260096\n",
      "  0.15217    -0.0313224   0.02495338 -0.18249337  0.30224973 -0.07295356\n",
      "  0.06089938 -0.22843209  0.09339875 -0.17783354 -0.3984548  -0.18630166\n",
      "  0.11291532  0.37653613 -0.54873633 -0.12683901 -0.2276088   0.02342611\n",
      " -0.00651013 -0.09438138 -0.43746728 -0.17882256  0.39969882  0.10334597\n",
      "  0.3812022   0.03450602  0.23544358 -0.10788208 -0.08930669 -0.13853778\n",
      " -0.07729761  0.15135615  0.04560615 -0.0067206   0.25592563 -0.33827654\n",
      "  0.23257913 -0.1034184   0.40542823 -0.01122486  0.5898305   0.07493297\n",
      "  0.26466784  0.00566754 -0.23037347  0.11882636  0.11135449  0.2595674\n",
      " -0.44694158  0.17356648 -0.15948798  0.13537613 -0.07780755 -0.23676734\n",
      " -0.02999007  0.02116179 -0.14570062 -0.3708301   0.05268478  0.05652348\n",
      " -0.18376659  0.44221413  0.10718554  0.06867085 -0.04612948  0.17047438\n",
      " -0.23441014  0.05660091 -0.44829985  0.10400279  0.14745295 -0.04035011\n",
      " -0.2551832   0.00764927 -0.27312163  0.1116685   0.19969836  0.09983974\n",
      " -0.37455815  0.29553455 -0.0997178  -0.15286826  0.28376773  0.02433771\n",
      " -0.06321289 -0.04340593 -0.12382123  0.03297462  0.1936334  -0.33273318\n",
      "  0.19260286  0.32718194  0.04269735 -0.17966674 -0.13045794 -0.06558293\n",
      " -0.27080333  0.11491679  0.2862067  -0.0152447  -0.08628303 -0.35513282\n",
      " -0.3621356  -0.12767735  0.06739841  0.23626477 -0.35517055  0.23577982\n",
      "  0.19413598 -0.16279355 -0.05792663 -0.07263373 -0.09926235  0.10778493\n",
      " -0.00577391  0.20654635 -0.10012583  0.02820365  0.27039778  0.3647146\n",
      " -0.3780932   0.02848426 -0.20957363 -0.01258735 -0.15303251  0.20227699\n",
      " -0.12858787 -0.14431025 -0.39198992 -0.21369399  0.31434226  0.08308297\n",
      " -0.191426   -0.15456305  0.03914822 -0.25830102 -0.30973577  0.18418692\n",
      "  0.19041558  0.09139802  0.11002283  0.09761564  0.0278441  -0.4202527\n",
      " -0.5468673   0.04750119  0.01858381  0.12253642  0.25063366  0.26121593\n",
      " -0.05402125 -0.15523136  0.01593142  0.00407898 -0.03876705  0.1666532\n",
      "  0.05520244 -0.01056162 -0.17780097  0.20814228  0.07698603  0.08507378\n",
      " -0.10691792 -0.00833607  0.1522577   0.2960565   0.02201396  0.43787047\n",
      "  0.26914546 -0.12550493  0.01410506  0.11521325  0.57665414  0.16844307\n",
      "  0.23082484  0.06066452 -0.0905467  -0.02329759 -0.5032271   0.54600763\n",
      " -0.04175334  0.33714867  0.34037915  0.28131586 -0.45908517 -0.01523589\n",
      "  0.09081329 -0.01556987  0.09530684  0.15791383 -0.07934856 -0.17674069\n",
      "  0.11216937  0.00991454  0.2753401  -0.00781397  0.13219842 -0.02829073\n",
      "  0.4163118   0.07497297 -0.11250068 -0.19574259  0.41159442  0.06876695\n",
      " -0.25294778 -0.18837993 -0.17418347  0.06825287  0.0480326  -0.41485757]\n"
     ]
    }
   ],
   "source": [
    "print(ft_embedding['privacy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Finding Similar Words\n",
    "Let's see the words that are \"closest\" to `privacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('security', 0.5754725933074951),\n",
       " ('protecting', 0.4895274043083191),\n",
       " ('confidentiality', 0.4804401993751526),\n",
       " ('protections', 0.4253268837928772),\n",
       " ('balances', 0.38053423166275024),\n",
       " ('checks', 0.3802436590194702),\n",
       " ('protection', 0.36849451065063477),\n",
       " ('rights', 0.3631744682788849),\n",
       " ('concerns', 0.3385086953639984),\n",
       " ('personal', 0.33178526163101196)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will also need to get the words closest to a word\n",
    "ft_model.similar_by_word('privacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Visualizing Similar Words\n",
    "Each word has a vector of 300 mapped to it. We can use a technique called [t-distributed Stochastic Neighbor Embedding (TSNE)](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to visualize this high-dimensional data in 2 dimensions.\n",
    "\n",
    "The function below will take a word, such as `confidentiality`, find its most similar words, reduce each of those words' vectors into a 2-dimensional space using TSNE, and then plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    arr = np.empty((0,300), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+50, x_coords.max()+50)\n",
    "    plt.ylim(y_coords.min()+50, y_coords.max()+50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_closestwords_tsnescatterplot(ft_model, 'confidentiality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Aggregating Word Embeddings\n",
    "Now that we know how to vectorize a single word, we need a way to aggregate the vectors of every word in a given comment. This will give us a single vector for each comment instead of one vector for each word within a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 TF-IDF Mean Aggregation\n",
    "A better, albeit more complicated, approach is to calculate a weighted average. Below, we'll **term-frequnecy inverse document frequency (tf-idf)** scores to weight each word's vector before calculating the mean.\n",
    ">**term-frequency inverse document frequency (tf-idf)**:  Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query\n",
    "\n",
    "To calculate this for all of our comments and working group text, we define the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This class is designed for use as a transformer within a sklearn pipeline. The pipeline will call\n",
    "        the fit and transform instance methods.\n",
    "\n",
    "        The class attributes instantiate the fastext word embedding model.\n",
    "        This model makes a dictionary mapping unique words from the entire corpus to vectors of shape [300,].\n",
    "        The transform method uses tf-idf weighting to aggregate each word vector at the doc level.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Create an instance of the class with the chosen model.\n",
    "\n",
    "        Arguments:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        # let tokens be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "        tokens = [word_tokenize(s) for s in corpus]\n",
    "\n",
    "        #### Create Fast Text Model ####\n",
    "        # Set values for various parameters\n",
    "        print(\"=\"*80)\n",
    "        print(\"Learning word embeddings for FastText Model...\")\n",
    "        feature_size = 300    # Word vector dimensionality\n",
    "        window_context = 50   # Context window size\n",
    "        min_word_count = 3    # Minimum word count\n",
    "        sample = 1e-3         # Downsample setting for frequent words\n",
    "\n",
    "        ft_model = gensim.models.fasttext.FastText(tokens, size=feature_size, window=window_context,\n",
    "                                                   min_count=min_word_count,sample=sample, sg=1, iter=50)\n",
    "        ft_embedding = {w: vec for w, vec in zip(ft_model.wv.vocab.keys(), ft_model.wv.vectors)}\n",
    "        print(\"\\tDone learning word embeddings for FastText Model.\")\n",
    "        print(\"_\"*80)\n",
    "\n",
    "        self.model = ft_embedding\n",
    "        self.dim = len(next(iter(self.model.values())))\n",
    "        self.word2weight = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            When this method is called by the sklearn pipeline, it creates the tf_idf scores for the words.\n",
    "            These will be used by transform as weights when aggregating the vector representations of each\n",
    "            word at the instance level.\n",
    "\n",
    "        \"\"\"\n",
    "        # pass callable to analyzer to extract the sequence of features out of the instance.\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X_train)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(lambda: max_idf,[(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, X_train):\n",
    "        instances  = [[word for word in instance.split(\" \")] for instance in X_train]\n",
    "        embeddings =  np.array([np.mean([self.model[w] * self.word2weight[w]\n",
    "                                         for w in words if w in self.model] or\n",
    "                        [np.zeros(self.dim)], axis=0) for words in instances]).astype('float')\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class will implement all of the word vectorization steps for us. First, we'll retrain our word embedding model on the entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Learning word embeddings for FastText Model...\n",
      "\tDone learning word embeddings for FastText Model.\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#instantiate model\n",
    "vec = TfidfEmbeddingVectorizer(corpus = corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll apply the word embedding model to the instance and keyword corpora separately. \n",
    "\n",
    "Since we're going to do math with these vectors, we'll include another step that standardizes the vectors by removing the mean and scaling to unit variance. This step is a common requirement for many algorithms as they might behave poorly if the vectors do not more or less look like standard normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word embeddings for each instance\n",
    "X = instance_corpus\n",
    "vec.fit(X)\n",
    "X_vec = vec.transform(X)\n",
    "#scale\n",
    "scaler = StandardScaler()\n",
    "X_vec_scaled = scaler.fit_transform(X_vec)\n",
    "\n",
    "\n",
    "#get the word embeddings for each working group\n",
    "melted_keywords = pd.melt(keywords[['Clean Text','WG1','WG2','WG3','WG4']],id_vars='Clean Text').dropna().drop(labels='value',axis=1)\n",
    "wg_df = melted_keywords.groupby(by='variable')['Clean Text'].apply(lambda x: \" \".join(x))\n",
    "y = wg_df.values\n",
    "vec.fit(y)\n",
    "y_vec = vec.transform(y)\n",
    "#scale\n",
    "scaler = StandardScaler()\n",
    "y_vec_scaled = scaler.fit_transform(y_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calculating Similarity between a Comment and a Working Group\n",
    "Now that we've got vector representations of each working group and each instance of feedback, we can calculate the **cosine similairty** between an instance of feedback and each working group. \n",
    ">**cosine similarity**: this computes the L2-normalized dot product of vectors. That is, if $x$ and $y$ are row vectors, their cosine similarity $k$ is defined as:\n",
    "$$k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}$$\n",
    "This is called cosine similarity because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.\n",
    "\n",
    "We can calculate the cosine similarity very easily in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cos = cosine_similarity(X_vec_scaled,y_vec_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we returned a kernel matrix, which is an array with shape (n_samples_x, n_samples_x). We had 604 comments and 4 working groups, so the `shape` of d_cos should be $(604,4)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_cos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this matrix represents and instance of feedback, and each column represents a working group. The values are the cosine similarity scores. Let's convert this array to a pandas dataframe to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010772</td>\n",
       "      <td>-0.055226</td>\n",
       "      <td>-0.094894</td>\n",
       "      <td>0.103237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.096053</td>\n",
       "      <td>-0.107179</td>\n",
       "      <td>-0.061559</td>\n",
       "      <td>0.014675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.043883</td>\n",
       "      <td>0.068102</td>\n",
       "      <td>0.043570</td>\n",
       "      <td>-0.033022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136334</td>\n",
       "      <td>-0.168094</td>\n",
       "      <td>-0.052018</td>\n",
       "      <td>0.004485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187178</td>\n",
       "      <td>-0.154060</td>\n",
       "      <td>-0.217363</td>\n",
       "      <td>0.065685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.010772 -0.055226 -0.094894  0.103237\n",
       "1  0.096053 -0.107179 -0.061559  0.014675\n",
       "2 -0.043883  0.068102  0.043570 -0.033022\n",
       "3  0.136334 -0.168094 -0.052018  0.004485\n",
       "4  0.187178 -0.154060 -0.217363  0.065685"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d_cos).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores range from $−1$, meaning exactly opposite, to $1$ meaning exactly the same. $0$ indicates orthogonality or decorrelation, while in-between values indicate intermediate similarity or dissimilarity.\n",
    "\n",
    "Since our task is to find the most similar working group for each instance of feedback, we want to find the max column value for each row. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = np.amax(d_cos,axis=1)\n",
    "max_indices = np.argmax(d_cos,axis=1)\n",
    "max_cos = list(zip(max_indices,max_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create a Spreadsheet of the Results\n",
    "Since the end-user doesn't know Python and wouldn't want to share these results via a Jupyer Notebook, we can write our results to a user-friendly Excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_name_map = {0:'Enterprise Data Governance',\n",
    "               1:'Access, Use and Augmentation',\n",
    "               2:'Decision-Making and Accountability',\n",
    "               3:'Commercialization, Innovation, and Public Use'}\n",
    "\n",
    "data['Recommended Working Group'] = np.nan\n",
    "data['Recommended Working Group Similarity Score'] = np.nan\n",
    "\n",
    "data[['Recommended Working Group','Recommended Working Group Similarity Score']] = max_cos\n",
    "data['Recommended Working Group'] = data['Recommended Working Group'].map(wg_name_map)\n",
    "\n",
    "data['Enterprise Data Governance Similarity Score'] = np.nan\n",
    "data['Access, Use and Augmentation Similarity Score'] = np.nan\n",
    "data['Decision-Making and Accountability Similarity Score'] = np.nan\n",
    "data['Commercialization, Innovation, and Public Use Similarity Score'] = np.nan\n",
    "data[['Enterprise Data Governance Similarity Score',\n",
    "      'Access, Use and Augmentation Similarity Score',\n",
    "      'Decision-Making and Accountability Similarity Score',\n",
    "      'Commercialization, Innovation, and Public Use Similarity Score']] = d_cos\n",
    "\n",
    "writer = pd.ExcelWriter('Instances Mapped to Working Groups.xlsx')\n",
    "data.to_excel(writer,'Sheet1',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Inspect the Results\n",
    "Below, we'll check out one of the feedback instances and see which working group it got mapped to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Recommended Working Group</th>\n",
       "      <th>Recommended Working Group Similarity Score</th>\n",
       "      <th>Enterprise Data Governance Similarity Score</th>\n",
       "      <th>Access, Use and Augmentation Similarity Score</th>\n",
       "      <th>Decision-Making and Accountability Similarity Score</th>\n",
       "      <th>Commercialization, Innovation, and Public Use Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I work as part of the team behind DataUSA.io, an open data visualization website which relies heavily on data from federal government sources (e.g...</td>\n",
       "      <td>Commercialization, Innovation, and Public Use</td>\n",
       "      <td>0.103237</td>\n",
       "      <td>0.010772</td>\n",
       "      <td>-0.055226</td>\n",
       "      <td>-0.094894</td>\n",
       "      <td>0.103237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#3 has 5 verbs\\n\\n#4 has 4 verbs, 2 adverbs, and 7 nouns and is basically a reiteration of OMB Circular A-130 and all the laws it implements.\\n\\n#...</td>\n",
       "      <td>Enterprise Data Governance</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>0.096053</td>\n",
       "      <td>-0.107179</td>\n",
       "      <td>-0.061559</td>\n",
       "      <td>0.014675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Integrate LEHD Data with Local/Regional Transportation data to develop more robust and timely \"Access to Jobs\" metrics. \\n\\nFor example: a low-inc...</td>\n",
       "      <td>Access, Use and Augmentation</td>\n",
       "      <td>0.068102</td>\n",
       "      <td>-0.043883</td>\n",
       "      <td>0.068102</td>\n",
       "      <td>0.043570</td>\n",
       "      <td>-0.033022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Previous engagement on government technology and data policy has occurred on GitHub -- with both the code and content available to all, and the fe...</td>\n",
       "      <td>Enterprise Data Governance</td>\n",
       "      <td>0.136334</td>\n",
       "      <td>0.136334</td>\n",
       "      <td>-0.168094</td>\n",
       "      <td>-0.052018</td>\n",
       "      <td>0.004485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is noticeable that there are no mentions of open data among the strategic principles, as this has been a strong aspect of previous U.S. governm...</td>\n",
       "      <td>Enterprise Data Governance</td>\n",
       "      <td>0.187178</td>\n",
       "      <td>0.187178</td>\n",
       "      <td>-0.154060</td>\n",
       "      <td>-0.217363</td>\n",
       "      <td>0.065685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                Instance  \\\n",
       "0  I work as part of the team behind DataUSA.io, an open data visualization website which relies heavily on data from federal government sources (e.g...   \n",
       "1  #3 has 5 verbs\\n\\n#4 has 4 verbs, 2 adverbs, and 7 nouns and is basically a reiteration of OMB Circular A-130 and all the laws it implements.\\n\\n#...   \n",
       "2  Integrate LEHD Data with Local/Regional Transportation data to develop more robust and timely \"Access to Jobs\" metrics. \\n\\nFor example: a low-inc...   \n",
       "3  Previous engagement on government technology and data policy has occurred on GitHub -- with both the code and content available to all, and the fe...   \n",
       "4  It is noticeable that there are no mentions of open data among the strategic principles, as this has been a strong aspect of previous U.S. governm...   \n",
       "\n",
       "                       Recommended Working Group  \\\n",
       "0  Commercialization, Innovation, and Public Use   \n",
       "1                     Enterprise Data Governance   \n",
       "2                   Access, Use and Augmentation   \n",
       "3                     Enterprise Data Governance   \n",
       "4                     Enterprise Data Governance   \n",
       "\n",
       "   Recommended Working Group Similarity Score  \\\n",
       "0                                    0.103237   \n",
       "1                                    0.096053   \n",
       "2                                    0.068102   \n",
       "3                                    0.136334   \n",
       "4                                    0.187178   \n",
       "\n",
       "   Enterprise Data Governance Similarity Score  \\\n",
       "0                                     0.010772   \n",
       "1                                     0.096053   \n",
       "2                                    -0.043883   \n",
       "3                                     0.136334   \n",
       "4                                     0.187178   \n",
       "\n",
       "   Access, Use and Augmentation Similarity Score  \\\n",
       "0                                      -0.055226   \n",
       "1                                      -0.107179   \n",
       "2                                       0.068102   \n",
       "3                                      -0.168094   \n",
       "4                                      -0.154060   \n",
       "\n",
       "   Decision-Making and Accountability Similarity Score  \\\n",
       "0                                            -0.094894   \n",
       "1                                            -0.061559   \n",
       "2                                             0.043570   \n",
       "3                                            -0.052018   \n",
       "4                                            -0.217363   \n",
       "\n",
       "   Commercialization, Innovation, and Public Use Similarity Score  \n",
       "0                                                        0.103237  \n",
       "1                                                        0.014675  \n",
       "2                                                       -0.033022  \n",
       "3                                                        0.004485  \n",
       "4                                                        0.065685  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
