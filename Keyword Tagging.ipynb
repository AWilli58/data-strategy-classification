{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federal Data Strategy Working Group Tagging\n",
    "This notebook contains code to identify the most relevant working group for each instance of feedback collected by the Federal Data Strategy.\n",
    "\n",
    "The output is a spreadsheet containing all of the feedback instances and similarity scores for each working group.\n",
    "\n",
    "Methodologies are documented inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim import corpora\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import enchant\n",
    "import contractions\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity, polynomial_kernel\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'/Users/scottmcallister/Desktop/GitHub/data-strategy-topic-modeling/Federal Data Strategy Comments.xlsx')\n",
    "keywords = pd.read_excel('Working_Group_Terms.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc, spellcheck=True):\n",
    "    \n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    \n",
    "    def strip_html_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text()\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_urls(text):\n",
    "        #url regex\n",
    "        url_re = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\")\n",
    "        stripped_text = url_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_emails(text):\n",
    "        #email address regex\n",
    "        email_re = re.compile(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)')\n",
    "        stripped_text = email_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_nonsense(text):\n",
    "        # leave words that are at least three characters long, do not contain a number, and are no more \n",
    "        # than 17 chars long\n",
    "        no_nonsense = re.findall(r'\\b[a-z][a-z][a-z]+\\b',text)\n",
    "        stripped_text = ' '.join(w for w in no_nonsense if w != 'nan' and len(w) <= 17)\n",
    "        return stripped_text\n",
    "\n",
    "    def expand_contractions(text, contraction_mapping=contractions.contractions_dict):\n",
    "\n",
    "            contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                              flags=re.IGNORECASE|re.DOTALL)\n",
    "            def expand_match(contraction):\n",
    "                match = contraction.group(0)\n",
    "                first_char = match[0]\n",
    "                if contraction_mapping.get(match):\n",
    "                    expanded_contraction = contraction_mapping.get(match)\n",
    "                else:\n",
    "                    expanded_contraction = contraction_mapping.get(match.lower())\n",
    "                if expanded_contraction:\n",
    "                    expanded_contraction = first_char+expanded_contraction[1:]\n",
    "                    return expanded_contraction\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "            expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "            return expanded_text\n",
    "\n",
    "    def strip_misspellings(text):\n",
    "        d = enchant.Dict(\"en_US\")\n",
    "        words_to_add = ['api','git','github','apis']\n",
    "        for w in words_to_add:\n",
    "            d.add(w)\n",
    "        \n",
    "        \n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        non_dict_words = set([word for word in tokens if d.check(word) is False and re.match('^[a-zA-Z ]*$',word)])\n",
    "        stripped_text = \" \".join([x for x in tokens if x not in non_dict_words])\n",
    "        return stripped_text\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    doc = \" \".join([word for word in tokenizer.tokenize(doc) if word not in en_stop])\n",
    "    contraction_free = expand_contractions(doc)\n",
    "    tag_free = strip_html_tags(contraction_free)\n",
    "    url_free = strip_urls(tag_free)\n",
    "    email_free = strip_emails(url_free)\n",
    "    if spellcheck:\n",
    "        misspelling_free = strip_misspellings(email_free)\n",
    "        normalized = strip_nonsense(misspelling_free)\n",
    "\n",
    "    else:\n",
    "        normalized = strip_nonsense(email_free)\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clean Instances'] = data['Instance'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_corpus = data['Clean Instances']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first concatenate the topic to its description\n",
    "keywords['Text'] = keywords['Topic'].fillna(value = \"\").astype(str) + \" \" + keywords['Description'].fillna(value = \"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this concatenated text, replacing np.nan with empty strings\n",
    "keywords['Clean Text'] = keywords['Text'].apply(lambda x: clean(x))\n",
    "keywords['Clean Text'].replace(r'',np.nan,regex=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_corpus = keywords['Clean Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combined Corpus\n",
    "corpus = pd.concat([keyword_corpus,instance_corpus]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the clean instances col\n",
    "data.drop(labels = 'Clean Instances',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This class is designed for use as a transformer within a sklearn pipeline. The pipeline will call\n",
    "        the fit and transform instance methods.\n",
    "\n",
    "        The class attributes instantiate the fastext word embedding model.\n",
    "        This model makes a dictionary mapping unique words from the entire corpus to vectors of shape [300,].\n",
    "        The transform method uses tf-idf weighting to aggregate each word vector at the doc level.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Create an instance of the class with the chosen model.\n",
    "\n",
    "        Arguments:\n",
    "            None\n",
    "\n",
    "        \"\"\"\n",
    "        # let tokens be a list of tokenized texts (i.e. list of lists of tokens)\n",
    "        tokens = [word_tokenize(s) for s in corpus]\n",
    "\n",
    "        #### Create Fast Text Model ####\n",
    "        # Set values for various parameters\n",
    "        print(\"=\"*80)\n",
    "        print(\"Learning word embeddings for FastText Model...\")\n",
    "        feature_size = 300    # Word vector dimensionality\n",
    "        window_context = 50   # Context window size\n",
    "        min_word_count = 3    # Minimum word count\n",
    "        sample = 1e-3         # Downsample setting for frequent words\n",
    "\n",
    "        ft_model = gensim.models.fasttext.FastText(tokens, size=feature_size, window=window_context,\n",
    "                                                   min_count=min_word_count,sample=sample, sg=1, iter=50)\n",
    "        ft_embedding = {w: vec for w, vec in zip(ft_model.wv.vocab.keys(), ft_model.wv.vectors)}\n",
    "        print(\"\\tDone learning word embeddings for FastText Model.\")\n",
    "        print(\"_\"*80)\n",
    "\n",
    "        self.model = ft_embedding\n",
    "        self.dim = len(next(iter(self.model.values())))\n",
    "        self.word2weight = None\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            When this method is called by the sklearn pipeline, it creates the tf_idf scores for the words.\n",
    "            These will be used by transform as weights when aggregating the vector representations of each\n",
    "            word at the instance level.\n",
    "\n",
    "        \"\"\"\n",
    "        # pass callable to analyzer to extract the sequence of features out of the instance.\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X_train)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(lambda: max_idf,[(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def transform(self, X_train):\n",
    "        instances  = [[word for word in instance.split(\" \")] for instance in X_train]\n",
    "        embeddings =  np.array([np.mean([self.model[w] * self.word2weight[w]\n",
    "                                         for w in words if w in self.model] or\n",
    "                        [np.zeros(self.dim)], axis=0) for words in instances]).astype('float')\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Learning word embeddings for FastText Model...\n",
      "\tDone learning word embeddings for FastText Model.\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#instantiate model\n",
    "vec = TfidfEmbeddingVectorizer(corpus = corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word embeddings for each instance\n",
    "X = instance_corpus\n",
    "vec.fit(X)\n",
    "X_vec = vec.transform(X)\n",
    "#scale\n",
    "scaler = StandardScaler()\n",
    "X_vec_scaled = scaler.fit_transform(X_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word embeddings for each working group\n",
    "melted_keywords = pd.melt(keywords[['Clean Text','WG1','WG2','WG3','WG4']],id_vars='Clean Text').dropna().drop(labels='value',axis=1)\n",
    "wg_df = melted_keywords.groupby(by='variable')['Clean Text'].apply(lambda x: \" \".join(x))\n",
    "y = wg_df.values\n",
    "vec.fit(y)\n",
    "y_vec = vec.transform(y)\n",
    "#scale\n",
    "scaler = StandardScaler()\n",
    "y_vec_scaled = scaler.fit_transform(y_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "`cosine_similarity` computes the L2-normalized dot product of vectors. That is, if $x$ and $y$ are row vectors, their cosine similarity $k$ is defined as:\n",
    "\n",
    "$$k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}$$\n",
    "\n",
    "This is called cosine similarity because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.\n",
    "\n",
    "The function returns a kernel matrix, which is an array with shape (n_samples_X, n_samples_Y).\n",
    "\n",
    "### Score Interpretation\n",
    "The resulting similarity scores range from $−1$ (meaning exactly opposite) to $1$ (meaning exactly the same). A $0$ indicates orthogonality or decorrelation, while in-between values indicate intermediate similarity or dissimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cos = cosine_similarity(X_vec_scaled,y_vec_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_values = np.amax(d_cos,axis=1)\n",
    "max_indices = np.argmax(d_cos,axis=1)\n",
    "max_cos = list(zip(max_indices,max_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_name_map = {0:'Enterprise Data Governance',\n",
    "               1:'Access, Use and Augmentation',\n",
    "               2:'Decision-Making and Accountability',\n",
    "               3:'Commercialization, Innovation, and Public Use'}\n",
    "\n",
    "data['Recommended Working Group'] = np.nan\n",
    "data['Recommended Working Group Similarity Score'] = np.nan\n",
    "\n",
    "data[['Recommended Working Group','Recommended Working Group Similarity Score']] = max_cos\n",
    "data['Recommended Working Group'] = data['Recommended Working Group'].map(wg_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Enterprise Data Governance Similarity Score'] = np.nan\n",
    "data['Access, Use and Augmentation Similarity Score'] = np.nan\n",
    "data['Decision-Making and Accountability Similarity Score'] = np.nan\n",
    "data['Commercialization, Innovation, and Public Use Similarity Score'] = np.nan\n",
    "data[['Enterprise Data Governance Similarity Score',\n",
    "      'Access, Use and Augmentation Similarity Score',\n",
    "      'Decision-Making and Accountability Similarity Score',\n",
    "      'Commercialization, Innovation, and Public Use Similarity Score']] = d_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Instances Mapped to Working Groups.xlsx')\n",
    "data.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
